require 'json'

current_dir    = File.dirname(File.expand_path(__FILE__))

if Vagrant::Util::Platform.windows?
  # Is Windows
  puts "Vagrant launched from windows."
  VMWARE_DISK_UTILITY="C:\\Program Files (x86)\\VMware\\VMware Workstation\\vmware-vdiskmanager.exe"
elsif Vagrant::Util::Platform.darwin?
  # Is Mac
  puts "Vagrant launched from mac."
  VMWARE_DISK_UTILITY="/System/Volumes/Data/Applications/VMware Fusion.app/Contents/Library/vmware-vdiskmanager"
else
  puts "Assuming Vagrant launched from Linux"
  VMWARE_DISK_UTILITY="/usr/bin/vmware-vdiskmanager"
end

puts "VMWARE_DISK_UTILITY: #{VMWARE_DISK_UTILITY}"

# Read Profile Config file
file = File.read('profile-config.json')
profile_config_json = JSON.parse(file)

# Read version file
file = File.read('../../versions.json')
kx_version_json = JSON.parse(file)
puts "KX Version JSON #{kx_version_json}"

STANDALONE_MODE = ENV['standaloneMode'] || profile_config_json['config']['standaloneMode']
puts "STANDALONE_MODE: #{STANDALONE_MODE}"

profile_config_local_volumes=profile_config_json['config']['local_volumes']
puts "Profile Config #{profile_config_local_volumes}"

if STANDALONE_MODE != "true"
    # Get needed capacity for GlusterFS volumes disk size
    GLUSTERFS_KUBE_VOLUMES_DISK_SIZE=profile_config_json['config']['glusterFsDiskSize'].to_i + 1
    puts "GlusterFS Disk Size: #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE}GB"
end

# Calculate needed local disk capacity
# Adding 2% and 4% respectively to cover space lost when creating PVs with Sig
NUM_1GB_LOCAL_SIZE=((profile_config_local_volumes['one_gb'].to_f * 1) * 1.04).round
NUM_5GB_LOCAL_SIZE=((profile_config_local_volumes['five_gb'].to_f * 5) * 1.02).round
NUM_10GB_LOCAL_SIZE=((profile_config_local_volumes['ten_gb'].to_f * 10) * 1.02).round
NUM_30GB_LOCAL_SIZE=((profile_config_local_volumes['thirty_gb'].to_f * 30) * 1.02).round
NUM_50GB_LOCAL_SIZE=((profile_config_local_volumes['fifty_gb'].to_f * 50) * 1.02).round

# Calculate disk size needs for requested local volumes configured in profile-config.json
LOCAL_KUBE_VOLUMES_DISK_SIZE=NUM_1GB_LOCAL_SIZE+NUM_5GB_LOCAL_SIZE+NUM_10GB_LOCAL_SIZE+NUM_30GB_LOCAL_SIZE+NUM_50GB_LOCAL_SIZE+1
puts "#{NUM_1GB_LOCAL_SIZE}+#{NUM_5GB_LOCAL_SIZE}+#{NUM_10GB_LOCAL_SIZE}+#{NUM_30GB_LOCAL_SIZE}+#{NUM_50GB_LOCAL_SIZE}+1"
puts "Local Disk Size: #{LOCAL_KUBE_VOLUMES_DISK_SIZE}GB"

# Environment variables will override the profile-config.json file properties. This was added for the Jenkins jobs, so that they can override parameters
ENVIRONMENT_PREFIX = ENV['environmentPrefix'] || profile_config_json['config']['environmentPrefix']
puts "ENVIRONMENT_PREFIX: #{ENVIRONMENT_PREFIX}"
MAIN_NODE_COUNT = ENV['mainNodeCount'] || profile_config_json['config']['vm_properties']['main_node_count']
puts "MAIN_NODE_COUNT: #{MAIN_NODE_COUNT}"
MAIN_ADMIN_NODE_CPU_CORES = ENV['adminMainNodeCpuCores'] || profile_config_json['config']['vm_properties']['main_admin_node_cpu_cores']
puts "MAIN_ADMIN_NODE_CPU_CORES: #{MAIN_ADMIN_NODE_CPU_CORES}"
MAIN_ADMIN_NODE_MEMORY = ENV['adminMainNodeMemory'] || profile_config_json['config']['vm_properties']['main_admin_node_memory']
puts "MAIN_ADMIN_NODE_MEMORY: #{MAIN_ADMIN_NODE_MEMORY}"
MAIN_REPLICA_NODE_CPU_CORES = ENV['replicaMainNodeCpuCores'] || profile_config_json['config']['vm_properties']['main_replica_node_cpu_cores']
puts "MAIN_REPLICA_NODE_CPU_CORES: #{MAIN_REPLICA_NODE_CPU_CORES}"
MAIN_REPLICA_NODE_MEMORY = ENV['replicaMainNodeMemory'] || profile_config_json['config']['vm_properties']['main_replica_node_memory']
puts "MAIN_REPLICA_NODE_MEMORY: #{MAIN_REPLICA_NODE_MEMORY}"
WORKER_NODE_COUNT = ENV['workerNodeCount'] || profile_config_json['config']['vm_properties']['worker_node_count']
puts "WORKER_NODE_COUNT: #{WORKER_NODE_COUNT}"
WORKER_NODE_CPU_CORES = ENV['workerNodeCpuCores'] || profile_config_json['config']['vm_properties']['worker_node_cpu_cores']
puts "WORKER_NODE_CPU_CORES: #{WORKER_NODE_CPU_CORES}"
WORKER_NODE_MEMORY = ENV['WorkerNodeMemory'] || profile_config_json['config']['vm_properties']['worker_node_memory']
puts "WORKER_NODE_MEMORY: #{WORKER_NODE_MEMORY}"
MAIN_BOX_VERSION = ENV['mainBoxVersion'] || kx_version_json['kxascode']
puts "MAIN_BOX_VERSION: #{MAIN_BOX_VERSION}"
NODE_BOX_VERSION = ENV['nodeBoxVersion'] || kx_version_json['kxascode']
puts "NODE_BOX_VERSION: #{NODE_BOX_VERSION}"
HASH = ENV['hash']
puts "HASH: #{HASH}"
KX_MAIN_BOX_LOCATION = ENV['kxMainBoxLocation'] || "base-vm/boxes/vmware-desktop-#{MAIN_BOX_VERSION}/kx-main-#{MAIN_BOX_VERSION}.box"
puts "KX_MAIN_BOX_LOCATION: #{KX_MAIN_BOX_LOCATION}"
KX_NODE_BOX_LOCATION = ENV['kxNodeBoxLocation'] || "base-vm/boxes/vmware-desktop-#{NODE_BOX_VERSION}/kx-node-#{NODE_BOX_VERSION}.box"
puts "KX_NODE_BOX_LOCATION: #{KX_NODE_BOX_LOCATION}"

WORKSPACE_HOME ||= :"../.."

if File.exist?("#{WORKSPACE_HOME}/#{KX_MAIN_BOX_LOCATION}")
    KX_MAIN_BOX_URL = "file://#{WORKSPACE_HOME}/#{KX_MAIN_BOX_LOCATION}"
    puts "Found local box #{KX_MAIN_BOX_URL}, using that instead of cloud"
else
    KX_MAIN_BOX_URL = "https://app.vagrantup.com/kxascode/boxes/kx-main"
    puts "Did not find local box #{KX_MAIN_BOX_LOCATION}, downloading box from cloud"
end

if File.exist?("#{WORKSPACE_HOME}/#{KX_NODE_BOX_LOCATION}")
    KX_NODE_BOX_URL = "file://#{WORKSPACE_HOME}/#{KX_NODE_BOX_LOCATION}"
    puts "Found local box #{KX_NODE_BOX_URL}, using that instead of cloud"
else
    KX_NODE_BOX_URL = "https://app.vagrantup.com/kxascode/boxes/kx-node"
    puts "Did not find local box #{KX_NODE_BOX_LOCATION}, downloading box from cloud"
end

if (MAIN_NODE_COUNT - 1) < 1
  MAIN_REPLICA_NODE_COUNT = 0
  puts "MAIN_REPLICA_NODE_COUNT < 1 -> #{MAIN_REPLICA_NODE_COUNT} replica main nodes will be provisioned"
else
  MAIN_REPLICA_NODE_COUNT = MAIN_NODE_COUNT - 1
  puts "MAIN_REPLICA_NODE_COUNT > 1 -> #{MAIN_REPLICA_NODE_COUNT} replica main nodes will be provisioned"
end

begin
  NAT_MAIN_GUEST_IP_ADDRESS = profile_config_json['config']['staticNetworkSetup']['baseFixedIpAddresses']['kx-main1']
rescue
  NAT_MAIN_GUEST_IP_ADDRESS = "0.0.0.0"
end
puts "NAT_MAIN_GUEST_IP_ADDRESS: #{NAT_MAIN_GUEST_IP_ADDRESS}"

# Path for current Jenkins solution
WORKSPACE_HOME = File.dirname(File.expand_path('../..', __FILE__))

Vagrant.configure("2") do |config|
    config.vm.define "kx-main1", primary: true do |subconfig|
        subconfig.vm.box = "kxascode/kx-main"
        if KX_MAIN_BOX_URL.start_with?("https://app.vagrantup.com/kxascode")
            subconfig.vm.box_version = "#{MAIN_BOX_VERSION}"
        end
        subconfig.vm.hostname = "kx-main1"
        subconfig.vm.network :forwarded_port, guest: 22, host: 2222, id: "ssh", disabled: true
        subconfig.vm.network :forwarded_port, guest: 22, host: 2230, id: "ssh-kx-main", auto_correct: true
        subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 5672, host_ip: "127.0.0.1", host: 5672, id: "rabbitmq"
        subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 15672, host_ip: "127.0.0.1", host: 15672, id: "rabbitmq-api"
        subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 3000, host_ip: "127.0.0.1", host: 8080, id: "kx-portal", auto_correct: true
        subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 443, host_ip: "127.0.0.1", host: 443, id: "k8s-ingress", auto_correct: true
        subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 8043, host_ip: "127.0.0.1", host: 8043, id: "guacamole-remote-desktop", auto_correct: true
        subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 5901, host_ip: "127.0.0.1", host: 5901, id: "tigervnc-remote-desktop", auto_correct: true
        subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 4000, host_ip: "127.0.0.1", host: 4000, id: "nomachine-remote-desktop", auto_correct: true
        subconfig.vm.box_url = "#{KX_MAIN_BOX_URL}"
        subconfig.vm.provision "shell", inline: "hostname -I | awk {'print $1'} | tr -d ' \\t\\n\\r' | sudo tee /vagrant/kx.as.code_main-ip-address"
        subconfig.vm.provision "shell", inline: "for i in {1..5}; do echo \"Waiting for /vagrant ... [\${i}\" of 5]; if [ -d /vagrant ]; then sudo cp -v -f /vagrant/*.json /usr/share/kx.as.code/workspace; break; else echo \"/vagrant not yet not available\"; sleep 5; fi; done"
        subconfig.vm.provision "shell", inline: "sudo mv -v -f /vagrant/.vmCredentialsFile /usr/share/kx.as.code/.config/ 2>/dev/null || true"
        subconfig.vm.provision "shell", inline: "sudo mkdir -p /usr/share/kx.as.code/workspace/custom-images"
        subconfig.vm.provision "shell", inline: "sudo cp -v -f /vagrant/avatar.{jpg,png} /vagrant/background.{jpg,png} /vagrant/boot.{jpg,png} /vagrant/conky_logo.{jpg,png} /vagrant/logo_icon.{jpg,png} /usr/share/kx.as.code/workspace/custom-images 2>/dev/null || true"
        subconfig.vm.provision "shell", inline: "jq '. + { \"state\": { \"kx_main1_ip_address\": \"'$(cat /vagrant/kx.as.code_main-ip-address)'\", \"provisioned_disks\": { \"local_storage_disk_size\": #{LOCAL_KUBE_VOLUMES_DISK_SIZE}, \"network_storage_disk_size\": #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE} } } }' /usr/share/kx.as.code/workspace/profile-config.json >/tmp/profile-config.json && mv /tmp/profile-config.json /usr/share/kx.as.code/workspace/profile-config.json"
        subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/profile-config.json | jq '.'"
        subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/users.json | jq '.'"
        subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/customVariables.json | jq '.'"
        subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/actionQueues.json | jq '.'"
        subconfig.vm.provision "shell", inline: 'echo $1 | sudo tee /var/tmp/.hash && sudo chmod 400 /var/tmp/.hash', :args => "#{HASH}"
        subconfig.vm.provision "shell", inline: 'echo -e "$(date "+%Y-%m-%d_%H%M%S") | KX-Main VM created by Vagrant" | sudo tee /usr/share/kx.as.code/workspace/gogogo'
        subconfig.trigger.after :destroy do |trigger|
        trigger.warn = "Destroying VM's disks"
            trigger.run = { :inline => "bash -c 'rm -f kx.as.code_main-ip-address kx-main-local-k8s-storage.vmdk kx-main-glusterfs-k8s-storage.vmdk'" }
        end
        subconfig.vm.provision :shell do |s|
          s.inline = "echo -e '{ \"DOCKERHUB_USER\": \"#{DOCKERHUB_USER}\", \"DOCKERHUB_EMAIL\": \"#{DOCKERHUB_EMAIL}\", \"DOCKERHUB_PASSWORD\": \"#{DOCKERHUB_PASSWORD}\" }' | sudo tee /var/tmp/.tmp.json"
        end
        subconfig.vm.provider "vmware_desktop" do |v|
            kx_main1_local_k8s_storage="#{current_dir}/kx-main1-local-k8s-storage.vmdk"
            if Vagrant::Util::Platform.windows?
                kx_main1_local_k8s_storage = kx_main1_local_k8s_storage.gsub(/\//, '\\')
            end
            if ARGV[0] == "up" && ! File.exist?(kx_main1_local_k8s_storage)
              `"#{VMWARE_DISK_UTILITY}" -c -s #{LOCAL_KUBE_VOLUMES_DISK_SIZE}GB -a lsilogic -t 0 #{kx_main1_local_k8s_storage}`
            end

            if STANDALONE_MODE != "true"
                kx_main1_glusterfs_k8s_storage="#{current_dir}/kx-main1-glusterfs-k8s-storage.vmdk"
                if Vagrant::Util::Platform.windows?
                    kx_main_glusterfs_k8s_storage = kx_main_glusterfs_k8s_storage.gsub(/\//, '\\')
                end
                if ARGV[0] == "up" && ! File.exist?(kx_main1_glusterfs_k8s_storage)
                  `"#{VMWARE_DISK_UTILITY}" -c -s #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE}GB -a lsilogic -t 0 #{kx_main1_glusterfs_k8s_storage}`
                end
            end

            v.gui = true
            v.whitelist_verified = true
            v.vmx['displayname'] = "kx.as.code-#{ENVIRONMENT_PREFIX}-main1-#{MAIN_BOX_VERSION}"
            v.vmx["memsize"] = "#{MAIN_ADMIN_NODE_MEMORY}"
            v.vmx["numvcpus"] = "#{MAIN_ADMIN_NODE_CPU_CORES}"
            v.vmx["mks.enable3d"] = "FALSE"
            v.vmx["ethernet0.pcislotnumber"] = "33"
            v.vmx["sound.startconnected"] = "FALSE"
            v.vmx["sound.present"] = "FALSE"
            v.vmx['scsi0:1.filename'] = kx_main1_local_k8s_storage
            v.vmx['scsi0:1.present']  = 'TRUE'
            v.vmx['scsi0:1.redo']     = ''
            v.vmx['scsi0:2.filename'] = kx_main1_glusterfs_k8s_storage
            v.vmx['scsi0:2.present']  = 'TRUE'
            v.vmx['scsi0:2.redo']     = ''

        end
    end

    (2..MAIN_NODE_COUNT).each do |i|
      config.vm.define "kx-main#{i}", primary: false, autostart: false do |subconfig|
        subconfig.vm.box = "kxascode/kx-node"
        if KX_NODE_BOX_URL.start_with?("https://app.vagrantup.com/kxascode")
            subconfig.vm.box_version = "#{NODE_BOX_VERSION}"
        end
        subconfig.vm.hostname = "kx-main#{i}"
        subconfig.vm.network :forwarded_port, guest: 22, host: 2222, id: "ssh", disabled: true
        subconfig.vm.network :forwarded_port, guest: 22, host: "224#{i}", id: "ssh-kx-main#{i}", auto_correct: true
        subconfig.vm.box_url = "#{KX_NODE_BOX_URL}"
        subconfig.vm.provision "shell", inline: "while [ ! -f /vagrant/kx.as.code_main-ip-address ]; do echo \"Waiting for kx.as.code_main-ip-address\" && sleep 15; done; cp /vagrant/kx.as.code_main-ip-address /var/tmp/"
        subconfig.vm.provision "shell", inline: "for i in {1..5}; do echo \"Waiting for /vagrant ... [\${i}\" of 5]; if [ -d /vagrant ]; then sudo cp -v -f /vagrant/profile-config.json /usr/share/kx.as.code/workspace; break; else echo \"/vagrant not yet not available\"; sleep 5; fi; done"
        subconfig.vm.provision "shell", inline: "jq '. + { \"state\": { \"kx_main1_ip_address\": \"'$(cat /vagrant/kx.as.code_main-ip-address)'\", \"provisioned_disks\": { \"local_storage_disk_size\": #{LOCAL_KUBE_VOLUMES_DISK_SIZE}, \"network_storage_disk_size\": #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE} } } }' /usr/share/kx.as.code/workspace/profile-config.json >/tmp/profile-config.json && mv /tmp/profile-config.json /usr/share/kx.as.code/workspace/profile-config.json"
        subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/profile-config.json | jq '.'"
        subconfig.vm.provision "shell", inline: 'echo -e "$(date "+%Y-%m-%d_%H%M%S") | KX-Main VM created by Vagrant" | sudo tee /usr/share/kx.as.code/workspace/gogogo'
        subconfig.trigger.after :destroy do |trigger|
          trigger.warn = "Destroying VM's disks"
              trigger.run = { :inline => "bash -c 'rm -f kx-main#{i}-local-k8s-storage.vmdk'" }
          end
          subconfig.vm.provider "vmware_desktop" do |v|

            kx_main_local_k8s_storage="#{current_dir}/kx-main#{i}-local-k8s-storage.vmdk"
            if Vagrant::Util::Platform.windows?
                kx_main_local_k8s_storage = kx_main_local_k8s_storage.gsub(/\//, '\\')
            end

            if ARGV[0] == "up" && ! File.exist?(kx_main_local_k8s_storage)
              `"#{VMWARE_DISK_UTILITY}" -c -s #{LOCAL_KUBE_VOLUMES_DISK_SIZE}GB -a lsilogic -t 0 #{kx_main_local_k8s_storage}`
            end

            v.gui = true
            v.whitelist_verified = true
            v.vmx['displayname'] = "kx.as.code-#{ENVIRONMENT_PREFIX}-main#{i}-#{NODE_BOX_VERSION}"
            v.vmx["memsize"] = "#{MAIN_REPLICA_NODE_MEMORY}"
            v.vmx["numvcpus"] = "#{MAIN_REPLICA_NODE_CPU_CORES}"
            v.vmx["mks.enable3d"] = "FALSE"
            v.vmx["ethernet0.pcislotnumber"] = "33"
            v.vmx["sound.startconnected"] = "FALSE"
            v.vmx["sound.present"] = "FALSE"
            v.vmx['scsi0:1.filename'] = kx_main_local_k8s_storage
            v.vmx['scsi0:1.present']  = 'TRUE'
            v.vmx['scsi0:1.redo']     = ''
        end
      end
    end

    (1..WORKER_NODE_COUNT).each do |i|
        config.vm.define "kx-worker#{i}" do |subconfig|
          subconfig.vm.box = "kxascode/kx-node"
          if !File.exist?("#{WORKSPACE_HOME}/#{KX_NODE_BOX_LOCATION}")
              subconfig.vm.box_version = "#{NODE_BOX_VERSION}"
          end
          subconfig.vm.hostname = "kx-worker#{i}"
          subconfig.vm.network :forwarded_port, guest: 22, host: 2222, id: "ssh", disabled: true
          subconfig.vm.network :forwarded_port, guest: 22, host: "223#{i}", id: "ssh-kx-worker#{i}", auto_correct: true
          subconfig.vm.box_url = "#{KX_NODE_BOX_URL}"
          subconfig.vm.provision "shell", inline: "while [ ! -f /vagrant/kx.as.code_main-ip-address ]; do echo \"Waiting for kx.as.code_main-ip-address\" && sleep 15; done; cp /vagrant/kx.as.code_main-ip-address /var/tmp/"
          subconfig.vm.provision "shell", inline: "for i in {1..5}; do echo \"Waiting for /vagrant ... [\${i}\" of 5]; if [ -d /vagrant ]; then sudo cp -v -f /vagrant/profile-config.json /usr/share/kx.as.code/workspace; break; else echo \"/vagrant not yet not available\"; sleep 5; fi; done"
          subconfig.vm.provision "shell", inline: "jq '. + { \"state\": { \"kx_main1_ip_address\": \"'$(cat /vagrant/kx.as.code_main-ip-address)'\", \"provisioned_disks\": { \"local_storage_disk_size\": #{LOCAL_KUBE_VOLUMES_DISK_SIZE}, \"network_storage_disk_size\": #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE} } } }' /usr/share/kx.as.code/workspace/profile-config.json >/tmp/profile-config.json && mv /tmp/profile-config.json /usr/share/kx.as.code/workspace/profile-config.json"
          subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/profile-config.json | jq '.'"
          subconfig.vm.provision "shell", inline: 'echo -e "$(date "+%Y-%m-%d_%H%M%S") | KX-Worker VM created by Vagrant" | sudo tee /usr/share/kx.as.code/workspace/gogogo'
          subconfig.trigger.after :destroy do |trigger|
            trigger.warn = "Destroying VM's disks"
            trigger.run = { :inline => "bash -c 'rm -f kx-worker#{i}-local-k8s-storage.vmdk'" }
          end
          subconfig.vm.provider "vmware_desktop" do |v|

            kx_worker_local_k8s_storage="#{current_dir}/kx-worker#{i}-local-k8s-storage.vmdk"
            if Vagrant::Util::Platform.windows?
                kx_worker_local_k8s_storage = kx_worker_local_k8s_storage.gsub(/\//, '\\')
            end

            if ARGV[0] == "up" && ! File.exist?(kx_worker_local_k8s_storage)
              `"#{VMWARE_DISK_UTILITY}" -c -s #{LOCAL_KUBE_VOLUMES_DISK_SIZE}GB -a lsilogic -t 0 #{kx_worker_local_k8s_storage}`
            end

            v.gui = true
            v.whitelist_verified = true
            v.vmx['displayname'] = "kx.as.code-#{ENVIRONMENT_PREFIX}-worker#{i}-#{NODE_BOX_VERSION}"
            v.vmx["memsize"] = "#{WORKER_NODE_MEMORY}"
            v.vmx["numvcpus"] = "#{WORKER_NODE_CPU_CORES}"
            v.vmx["mks.enable3d"] = "FALSE"
            v.vmx["ethernet0.pcislotnumber"] = "33"
            v.vmx["sound.startconnected"] = "FALSE"
            v.vmx["sound.present"] = "FALSE"
            v.vmx['scsi0:1.filename'] = kx_worker_local_k8s_storage
            v.vmx['scsi0:1.present']  = 'TRUE'
            v.vmx['scsi0:1.redo']     = ''
          end
        end
    end
end
